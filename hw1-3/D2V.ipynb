{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import gensim\n",
    "from nltk.tokenize import word_tokenize\n",
    "import xml.etree.ElementTree as ET\n",
    "from lxml import etree\n",
    "parser = etree.XMLParser(recover=True)\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "data_path = 't3-doc/'\n",
    "dirs = os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Get information of t3-doc/\n",
    "data = {}\n",
    "for xml in os.listdir(data_path):\n",
    "    with open(data_path + xml) as f:\n",
    "        index = int(xml.split('.xml')[0])\n",
    "        data[index] = {}\n",
    "        tmp = f.read().split('\\n\\n')\n",
    "        date = etree.fromstringlist(['<date>', tmp[0], '</date>'], parser=parser)[0].text\n",
    "        title = etree.fromstringlist(['<title>', tmp[1], '</title>'], parser=parser)[0].text\n",
    "        try:\n",
    "            abstract = etree.fromstringlist(['<abstract>', tmp[2], '</abstract>'], parser=parser)[0].text\n",
    "        except:\n",
    "            abstract = etree.fromstringlist(['<abstract>', tmp[3], '</abstract>'], parser=parser)[0].text\n",
    "        data[index]['date'] = date\n",
    "        data[index]['title'] = title\n",
    "        data[index]['abstract'] = abstract\n",
    "        \n",
    "\n",
    "alltitle = [data[i]['title'].strip() for i in sorted(data)]\n",
    "allabs = [data[i]['abstract'].strip() for i in sorted(data)]\n",
    "alldate = [data[i]['date'].strip() for i in sorted(data)]\n",
    "#tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(alldata)]\n",
    "tagged_title = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(alltitle)]\n",
    "tagged_abs = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(allabs)]\n",
    "doc_n = len(alltitle)\n",
    "\n",
    "#Get date information (int)\n",
    "from datetime import datetime\n",
    "\n",
    "for i in range(len(alldate)):\n",
    "    date = alldate[i]\n",
    "    #print(date)\n",
    "    for fmt in ['%a, %d %b %Y', '%a, %d %b %y']:\n",
    "        try:\n",
    "            tmp = datetime.strptime(date, fmt)\n",
    "        except ValueError as v:\n",
    "            if 'unconverted' in v.args[0]:\n",
    "                ulr = len(v.args[0].partition('unconverted data remains: ')[2])\n",
    "                if ulr:\n",
    "                    date = date[:-ulr]\n",
    "                    try:\n",
    "                        tmp = datetime.strptime(date, fmt)\n",
    "                        break\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "                else:\n",
    "                    raise v\n",
    "            else:\n",
    "                pass\n",
    "    alldate[i] = int(datetime.strftime(tmp, '%Y%m%d'))\n",
    "    #print(alldate[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Calculate Doc Sims\n",
    "\n",
    "if \"sims_title.npy\" not in dirs or \"sims_abs.npy\" not in dirs:\n",
    "    gen_titles = [[w.lower() for w in word_tokenize(t)] for t in alltitle]\n",
    "    gen_abss = [[w.lower() for w in word_tokenize(t)] for t in allabs]\n",
    "    dict_title = gensim.corpora.dictionary.Dictionary(gen_titles)\n",
    "    dict_abs = gensim.corpora.dictionary.Dictionary(gen_abss)\n",
    "    corpus_title = [dict_title.doc2bow(gen_title) for gen_title in gen_titles]\n",
    "    corpus_abs = [dict_title.doc2bow(gen_abs) for gen_abs in gen_abss]\n",
    "    tf_idf_title = gensim.models.TfidfModel(corpus_title)\n",
    "    tf_idf_abs = gensim.models.TfidfModel(corpus_abs)\n",
    "    sims_title = gensim.similarities.Similarity('sim',tf_idf_title[corpus_title], \n",
    "                                            num_features=len(dict_title))\n",
    "    sims_abs = gensim.similarities.Similarity('sim',tf_idf_abs[corpus_abs], \n",
    "                                            num_features=len(dict_abs))\n",
    "    tmp1 = []\n",
    "    tmp2 = []\n",
    "    for line in sims_title:\n",
    "        tmp1.append(line)\n",
    "    for line in sims_abs:\n",
    "        tmp2.append(line)\n",
    "    del sims_title, sims_abs\n",
    "    sims_title = np.array(tmp1)\n",
    "    sims_abs = np.array(tmp2)\n",
    "    del tmp1, tmp2\n",
    "    np.save('sims_title.npy', sims_title)\n",
    "    np.save('sims_abs.npy', sims_abs)\n",
    "else:\n",
    "    sims_title = np.load('sims_title.npy')\n",
    "    sims_abs = np.load('sims_abs.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#D2V\n",
    "\n",
    "if \"d2v_t.model\" not in dirs or \"d2v_a.model\" not in dirs:\n",
    "    max_epochs = 100\n",
    "    vec_size = 64\n",
    "    alpha = 0.025\n",
    "\n",
    "    model_title = Doc2Vec(size=vec_size,\n",
    "                    alpha=alpha, \n",
    "                    min_alpha=0.00025,\n",
    "                    min_count=5,\n",
    "                    workers=10,\n",
    "                    dm =1)\n",
    "\n",
    "    model_title.build_vocab(tagged_title)\n",
    "\n",
    "    model_abs = Doc2Vec(size=vec_size,\n",
    "                    alpha=alpha, \n",
    "                    min_alpha=0.00025,\n",
    "                    min_count=5,\n",
    "                    workers=10,\n",
    "                    dm =1)\n",
    "\n",
    "    model_abs.build_vocab(tagged_abs)\n",
    "\n",
    "\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        print('iteration {0}'.format(epoch))\n",
    "        model_title.train(tagged_title,\n",
    "                    total_examples=model_title.corpus_count,\n",
    "                    epochs=model_title.iter)\n",
    "        model_abs.train(tagged_abs,\n",
    "                    total_examples=model_abs.corpus_count,\n",
    "                    epochs=model_abs.iter)\n",
    "        # decrease the learning rate\n",
    "        model_title.alpha -= 0.0002\n",
    "        model_abs.alpha -= 0.0002\n",
    "        # fix the learning rate, no decay\n",
    "        model_title.min_alpha = model_title.alpha\n",
    "        model_abs.min_alpha = model_abs.alpha\n",
    "\n",
    "    model_title.save(\"d2v_t.model\")\n",
    "    model_abs.save(\"d2v_a.model\")\n",
    "    print(\"Model Saved\")\n",
    "else:\n",
    "    model_title = Doc2Vec.load('d2v_t.model')\n",
    "    model_abs = Doc2Vec.load('d2v_a.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(815719, 2)\n"
     ]
    }
   ],
   "source": [
    "#Negative Sampling\n",
    "import networkx as nx\n",
    "G = nx.read_edgelist('t3-train.txt', nodetype=int, create_using=nx.DiGraph())\n",
    "\n",
    "#negative sampling\n",
    "neg = []\n",
    "for node in G:\n",
    "    for nbr, datadict in G.adj[node].items():\n",
    "        for nnbr, datadict in G.adj[nbr].items():\n",
    "            if nnbr not in G.adj[node]:\n",
    "                neg.append(np.array([node, nnbr]))\n",
    "neg = np.array(neg)\n",
    "print(neg.shape)\n",
    "np.random.shuffle(neg)\n",
    "pos_num = sum([1 for line in open('t3-train.txt')])\n",
    "neg_sample = neg[:pos_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17500, 128)\n"
     ]
    }
   ],
   "source": [
    "#build embeddings dict\n",
    "embeddings = []\n",
    "for i in range(len(tagged_title)):\n",
    "    title_emb = model_title.infer_vector(alltitle[i])\n",
    "    abs_emb = model_abs.infer_vector(allabs[i])\n",
    "    embeddings.append(np.hstack((title_emb, abs_emb)))\n",
    "    #embeddings.append(model.infer_vector(alldata[i]))\n",
    "embeddings = np.array(embeddings)\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dada/.local/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Split training/testing data\n",
    "#scaler = StandardScaler()\n",
    "X = []\n",
    "y = []\n",
    "with open('t3-train.txt') as f:\n",
    "    for line in f:\n",
    "        id1, id2 = map(int, line.split())\n",
    "        emb1 = embeddings[id1-1]\n",
    "        emb2 = embeddings[id2-1]\n",
    "        X.append(np.hstack((emb1, emb2, alldate[id1-1], alldate[id2-1],\n",
    "                            sims_title[id1-1][id2-1], sims_abs[id1-1][id2-1])))\n",
    "        y.append(1)\n",
    "    for id1, id2 in neg_sample:\n",
    "        emb1 = embeddings[id1-1]\n",
    "        emb2 = embeddings[id2-1]\n",
    "        X.append(np.hstack((emb1, emb2, alldate[id1-1], alldate[id2-1],\n",
    "                            sims_title[id1-1][id2-1], sims_abs[id1-1][id2-1])))\n",
    "        y.append(0)\n",
    "#scaler.fit(X)\n",
    "X = np.array(X)\n",
    "\n",
    "y = np.array(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.644239\tval-logloss:0.6436\n",
      "Multiple eval metrics have been passed: 'val-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until val-logloss hasn't improved in 10 rounds.\n",
      "[1]\ttrain-logloss:0.617355\tval-logloss:0.617322\n",
      "[2]\ttrain-logloss:0.603824\tval-logloss:0.603682\n",
      "[3]\ttrain-logloss:0.594511\tval-logloss:0.594986\n",
      "[4]\ttrain-logloss:0.584159\tval-logloss:0.585384\n",
      "[5]\ttrain-logloss:0.577993\tval-logloss:0.579356\n",
      "[6]\ttrain-logloss:0.574453\tval-logloss:0.576494\n",
      "[7]\ttrain-logloss:0.571396\tval-logloss:0.573583\n",
      "[8]\ttrain-logloss:0.568035\tval-logloss:0.570097\n",
      "[9]\ttrain-logloss:0.565637\tval-logloss:0.567599\n",
      "[10]\ttrain-logloss:0.564095\tval-logloss:0.566293\n",
      "[11]\ttrain-logloss:0.56304\tval-logloss:0.565381\n",
      "[12]\ttrain-logloss:0.561412\tval-logloss:0.564269\n",
      "[13]\ttrain-logloss:0.558828\tval-logloss:0.561755\n",
      "[14]\ttrain-logloss:0.55729\tval-logloss:0.560557\n",
      "[15]\ttrain-logloss:0.555953\tval-logloss:0.559459\n",
      "[16]\ttrain-logloss:0.554937\tval-logloss:0.558666\n",
      "[17]\ttrain-logloss:0.553723\tval-logloss:0.557572\n",
      "[18]\ttrain-logloss:0.552456\tval-logloss:0.556396\n",
      "[19]\ttrain-logloss:0.551687\tval-logloss:0.555731\n",
      "[20]\ttrain-logloss:0.551083\tval-logloss:0.55517\n",
      "[21]\ttrain-logloss:0.550208\tval-logloss:0.554566\n",
      "[22]\ttrain-logloss:0.549559\tval-logloss:0.553863\n",
      "[23]\ttrain-logloss:0.548564\tval-logloss:0.552969\n",
      "[24]\ttrain-logloss:0.547879\tval-logloss:0.552597\n",
      "[25]\ttrain-logloss:0.547228\tval-logloss:0.552075\n",
      "[26]\ttrain-logloss:0.546189\tval-logloss:0.551584\n",
      "[27]\ttrain-logloss:0.545251\tval-logloss:0.550389\n",
      "[28]\ttrain-logloss:0.544125\tval-logloss:0.549368\n",
      "[29]\ttrain-logloss:0.543548\tval-logloss:0.548737\n",
      "[30]\ttrain-logloss:0.542943\tval-logloss:0.548092\n",
      "[31]\ttrain-logloss:0.542436\tval-logloss:0.547499\n",
      "[32]\ttrain-logloss:0.541906\tval-logloss:0.547119\n",
      "[33]\ttrain-logloss:0.541365\tval-logloss:0.546865\n",
      "[34]\ttrain-logloss:0.540847\tval-logloss:0.546401\n",
      "[35]\ttrain-logloss:0.539986\tval-logloss:0.545725\n",
      "[36]\ttrain-logloss:0.539473\tval-logloss:0.545408\n",
      "[37]\ttrain-logloss:0.539024\tval-logloss:0.545154\n",
      "[38]\ttrain-logloss:0.53854\tval-logloss:0.544835\n",
      "[39]\ttrain-logloss:0.538086\tval-logloss:0.544607\n",
      "[40]\ttrain-logloss:0.537271\tval-logloss:0.544102\n",
      "[41]\ttrain-logloss:0.536488\tval-logloss:0.543458\n",
      "[42]\ttrain-logloss:0.53596\tval-logloss:0.542981\n",
      "[43]\ttrain-logloss:0.535388\tval-logloss:0.54277\n",
      "[44]\ttrain-logloss:0.534933\tval-logloss:0.542412\n",
      "[45]\ttrain-logloss:0.53396\tval-logloss:0.541646\n",
      "[46]\ttrain-logloss:0.533594\tval-logloss:0.541312\n",
      "[47]\ttrain-logloss:0.533257\tval-logloss:0.541013\n",
      "[48]\ttrain-logloss:0.532991\tval-logloss:0.540729\n",
      "[49]\ttrain-logloss:0.53253\tval-logloss:0.540546\n",
      "[50]\ttrain-logloss:0.532241\tval-logloss:0.540479\n",
      "[51]\ttrain-logloss:0.53193\tval-logloss:0.54032\n",
      "[52]\ttrain-logloss:0.531591\tval-logloss:0.540151\n",
      "[53]\ttrain-logloss:0.53128\tval-logloss:0.539851\n",
      "[54]\ttrain-logloss:0.5309\tval-logloss:0.539499\n",
      "[55]\ttrain-logloss:0.530655\tval-logloss:0.539368\n",
      "[56]\ttrain-logloss:0.530409\tval-logloss:0.539241\n",
      "[57]\ttrain-logloss:0.5302\tval-logloss:0.539139\n",
      "[58]\ttrain-logloss:0.529887\tval-logloss:0.538939\n",
      "[59]\ttrain-logloss:0.529334\tval-logloss:0.538515\n",
      "[60]\ttrain-logloss:0.529086\tval-logloss:0.53842\n",
      "[61]\ttrain-logloss:0.528683\tval-logloss:0.538292\n",
      "[62]\ttrain-logloss:0.528311\tval-logloss:0.538142\n",
      "[63]\ttrain-logloss:0.527853\tval-logloss:0.537812\n",
      "[64]\ttrain-logloss:0.52733\tval-logloss:0.537554\n",
      "[65]\ttrain-logloss:0.527159\tval-logloss:0.537493\n",
      "[66]\ttrain-logloss:0.526916\tval-logloss:0.537318\n",
      "[67]\ttrain-logloss:0.52653\tval-logloss:0.536929\n",
      "[68]\ttrain-logloss:0.526319\tval-logloss:0.536855\n",
      "[69]\ttrain-logloss:0.525918\tval-logloss:0.536611\n",
      "[70]\ttrain-logloss:0.52575\tval-logloss:0.536535\n",
      "[71]\ttrain-logloss:0.525346\tval-logloss:0.536282\n",
      "[72]\ttrain-logloss:0.525003\tval-logloss:0.535902\n",
      "[73]\ttrain-logloss:0.524607\tval-logloss:0.53589\n",
      "[74]\ttrain-logloss:0.524292\tval-logloss:0.535809\n",
      "[75]\ttrain-logloss:0.523882\tval-logloss:0.5354\n",
      "[76]\ttrain-logloss:0.523735\tval-logloss:0.535457\n",
      "[77]\ttrain-logloss:0.523303\tval-logloss:0.535245\n",
      "[78]\ttrain-logloss:0.523118\tval-logloss:0.535166\n",
      "[79]\ttrain-logloss:0.522672\tval-logloss:0.535015\n",
      "[80]\ttrain-logloss:0.522289\tval-logloss:0.534561\n",
      "[81]\ttrain-logloss:0.521732\tval-logloss:0.534163\n",
      "[82]\ttrain-logloss:0.521368\tval-logloss:0.533986\n",
      "[83]\ttrain-logloss:0.521059\tval-logloss:0.533736\n",
      "[84]\ttrain-logloss:0.520851\tval-logloss:0.533525\n",
      "[85]\ttrain-logloss:0.520462\tval-logloss:0.533211\n",
      "[86]\ttrain-logloss:0.520187\tval-logloss:0.532999\n",
      "[87]\ttrain-logloss:0.519786\tval-logloss:0.532708\n",
      "[88]\ttrain-logloss:0.519385\tval-logloss:0.532358\n",
      "[89]\ttrain-logloss:0.519095\tval-logloss:0.532427\n",
      "[90]\ttrain-logloss:0.518792\tval-logloss:0.53223\n",
      "[91]\ttrain-logloss:0.51865\tval-logloss:0.532085\n",
      "[92]\ttrain-logloss:0.518259\tval-logloss:0.531874\n",
      "[93]\ttrain-logloss:0.518016\tval-logloss:0.531715\n",
      "[94]\ttrain-logloss:0.517675\tval-logloss:0.531515\n",
      "[95]\ttrain-logloss:0.51737\tval-logloss:0.531353\n",
      "[96]\ttrain-logloss:0.517144\tval-logloss:0.531141\n",
      "[97]\ttrain-logloss:0.516873\tval-logloss:0.53094\n",
      "[98]\ttrain-logloss:0.516606\tval-logloss:0.530939\n",
      "[99]\ttrain-logloss:0.516245\tval-logloss:0.530876\n",
      "[100]\ttrain-logloss:0.515965\tval-logloss:0.530755\n",
      "[101]\ttrain-logloss:0.515673\tval-logloss:0.530407\n",
      "[102]\ttrain-logloss:0.515411\tval-logloss:0.530315\n",
      "[103]\ttrain-logloss:0.515173\tval-logloss:0.530096\n",
      "[104]\ttrain-logloss:0.514863\tval-logloss:0.52976\n",
      "[105]\ttrain-logloss:0.514654\tval-logloss:0.529625\n",
      "[106]\ttrain-logloss:0.514338\tval-logloss:0.529464\n",
      "[107]\ttrain-logloss:0.514155\tval-logloss:0.529351\n",
      "[108]\ttrain-logloss:0.513868\tval-logloss:0.529308\n",
      "[109]\ttrain-logloss:0.513689\tval-logloss:0.529192\n",
      "[110]\ttrain-logloss:0.513481\tval-logloss:0.52916\n",
      "[111]\ttrain-logloss:0.513237\tval-logloss:0.529149\n",
      "[112]\ttrain-logloss:0.512993\tval-logloss:0.529081\n",
      "[113]\ttrain-logloss:0.512782\tval-logloss:0.528987\n",
      "[114]\ttrain-logloss:0.512658\tval-logloss:0.528976\n",
      "[115]\ttrain-logloss:0.512384\tval-logloss:0.528855\n",
      "[116]\ttrain-logloss:0.512137\tval-logloss:0.528844\n",
      "[117]\ttrain-logloss:0.511956\tval-logloss:0.528881\n",
      "[118]\ttrain-logloss:0.51175\tval-logloss:0.528834\n",
      "[119]\ttrain-logloss:0.511657\tval-logloss:0.528805\n",
      "[120]\ttrain-logloss:0.511328\tval-logloss:0.528624\n",
      "[121]\ttrain-logloss:0.510659\tval-logloss:0.528122\n",
      "[122]\ttrain-logloss:0.510358\tval-logloss:0.527824\n",
      "[123]\ttrain-logloss:0.5101\tval-logloss:0.52752\n",
      "[124]\ttrain-logloss:0.509879\tval-logloss:0.527413\n",
      "[125]\ttrain-logloss:0.509707\tval-logloss:0.527207\n",
      "[126]\ttrain-logloss:0.509592\tval-logloss:0.52715\n",
      "[127]\ttrain-logloss:0.509374\tval-logloss:0.527133\n",
      "[128]\ttrain-logloss:0.509054\tval-logloss:0.526845\n",
      "[129]\ttrain-logloss:0.508555\tval-logloss:0.526416\n",
      "[130]\ttrain-logloss:0.508352\tval-logloss:0.526347\n",
      "[131]\ttrain-logloss:0.508134\tval-logloss:0.526293\n",
      "[132]\ttrain-logloss:0.507929\tval-logloss:0.526271\n",
      "[133]\ttrain-logloss:0.507638\tval-logloss:0.526069\n",
      "[134]\ttrain-logloss:0.507386\tval-logloss:0.525992\n",
      "[135]\ttrain-logloss:0.507233\tval-logloss:0.525927\n",
      "[136]\ttrain-logloss:0.506792\tval-logloss:0.525543\n",
      "[137]\ttrain-logloss:0.506518\tval-logloss:0.525436\n",
      "[138]\ttrain-logloss:0.506353\tval-logloss:0.525392\n",
      "[139]\ttrain-logloss:0.506217\tval-logloss:0.525306\n",
      "[140]\ttrain-logloss:0.505895\tval-logloss:0.525157\n",
      "[141]\ttrain-logloss:0.505645\tval-logloss:0.524948\n",
      "[142]\ttrain-logloss:0.505542\tval-logloss:0.524807\n",
      "[143]\ttrain-logloss:0.505391\tval-logloss:0.52473\n",
      "[144]\ttrain-logloss:0.505139\tval-logloss:0.52455\n",
      "[145]\ttrain-logloss:0.505\tval-logloss:0.52457\n",
      "[146]\ttrain-logloss:0.504838\tval-logloss:0.524514\n",
      "[147]\ttrain-logloss:0.50461\tval-logloss:0.524523\n",
      "[148]\ttrain-logloss:0.504364\tval-logloss:0.524491\n",
      "[149]\ttrain-logloss:0.504153\tval-logloss:0.524384\n",
      "[150]\ttrain-logloss:0.503953\tval-logloss:0.524304\n",
      "[151]\ttrain-logloss:0.50364\tval-logloss:0.524242\n",
      "[152]\ttrain-logloss:0.503378\tval-logloss:0.523987\n",
      "[153]\ttrain-logloss:0.50311\tval-logloss:0.523731\n",
      "[154]\ttrain-logloss:0.502929\tval-logloss:0.523704\n",
      "[155]\ttrain-logloss:0.502631\tval-logloss:0.523485\n",
      "[156]\ttrain-logloss:0.502396\tval-logloss:0.523348\n",
      "[157]\ttrain-logloss:0.502249\tval-logloss:0.523206\n",
      "[158]\ttrain-logloss:0.502035\tval-logloss:0.523135\n",
      "[159]\ttrain-logloss:0.501769\tval-logloss:0.522946\n",
      "[160]\ttrain-logloss:0.501607\tval-logloss:0.523028\n",
      "[161]\ttrain-logloss:0.501433\tval-logloss:0.523025\n",
      "[162]\ttrain-logloss:0.501333\tval-logloss:0.522935\n",
      "[163]\ttrain-logloss:0.501137\tval-logloss:0.523009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[164]\ttrain-logloss:0.501022\tval-logloss:0.522981\n",
      "[165]\ttrain-logloss:0.500956\tval-logloss:0.522973\n",
      "[166]\ttrain-logloss:0.500702\tval-logloss:0.522812\n",
      "[167]\ttrain-logloss:0.5005\tval-logloss:0.522682\n",
      "[168]\ttrain-logloss:0.500272\tval-logloss:0.52251\n",
      "[169]\ttrain-logloss:0.500131\tval-logloss:0.522429\n",
      "[170]\ttrain-logloss:0.500017\tval-logloss:0.522452\n",
      "[171]\ttrain-logloss:0.499855\tval-logloss:0.522398\n",
      "[172]\ttrain-logloss:0.499674\tval-logloss:0.522217\n",
      "[173]\ttrain-logloss:0.499559\tval-logloss:0.522141\n",
      "[174]\ttrain-logloss:0.499354\tval-logloss:0.522241\n",
      "[175]\ttrain-logloss:0.499122\tval-logloss:0.522116\n",
      "[176]\ttrain-logloss:0.49892\tval-logloss:0.52201\n",
      "[177]\ttrain-logloss:0.498699\tval-logloss:0.521911\n",
      "[178]\ttrain-logloss:0.498451\tval-logloss:0.52181\n",
      "[179]\ttrain-logloss:0.498239\tval-logloss:0.521575\n",
      "[180]\ttrain-logloss:0.497997\tval-logloss:0.52144\n",
      "[181]\ttrain-logloss:0.497807\tval-logloss:0.521394\n",
      "[182]\ttrain-logloss:0.497651\tval-logloss:0.521362\n",
      "[183]\ttrain-logloss:0.497454\tval-logloss:0.521414\n",
      "[184]\ttrain-logloss:0.497285\tval-logloss:0.521444\n",
      "[185]\ttrain-logloss:0.497038\tval-logloss:0.521089\n",
      "[186]\ttrain-logloss:0.496963\tval-logloss:0.52091\n",
      "[187]\ttrain-logloss:0.496556\tval-logloss:0.520473\n",
      "[188]\ttrain-logloss:0.496334\tval-logloss:0.520495\n",
      "[189]\ttrain-logloss:0.496171\tval-logloss:0.520387\n",
      "[190]\ttrain-logloss:0.495984\tval-logloss:0.52038\n",
      "[191]\ttrain-logloss:0.495856\tval-logloss:0.520308\n",
      "[192]\ttrain-logloss:0.49566\tval-logloss:0.52022\n",
      "[193]\ttrain-logloss:0.495497\tval-logloss:0.520128\n",
      "[194]\ttrain-logloss:0.495288\tval-logloss:0.520134\n",
      "[195]\ttrain-logloss:0.495201\tval-logloss:0.520038\n",
      "[196]\ttrain-logloss:0.495002\tval-logloss:0.520035\n",
      "[197]\ttrain-logloss:0.494786\tval-logloss:0.520089\n",
      "[198]\ttrain-logloss:0.494617\tval-logloss:0.520209\n",
      "[199]\ttrain-logloss:0.494489\tval-logloss:0.520094\n",
      "[200]\ttrain-logloss:0.494362\tval-logloss:0.520086\n",
      "[201]\ttrain-logloss:0.49417\tval-logloss:0.519978\n",
      "[202]\ttrain-logloss:0.493977\tval-logloss:0.519962\n",
      "[203]\ttrain-logloss:0.493859\tval-logloss:0.519816\n",
      "[204]\ttrain-logloss:0.49368\tval-logloss:0.519664\n",
      "[205]\ttrain-logloss:0.493442\tval-logloss:0.519575\n",
      "[206]\ttrain-logloss:0.493229\tval-logloss:0.519592\n",
      "[207]\ttrain-logloss:0.493027\tval-logloss:0.519553\n",
      "[208]\ttrain-logloss:0.492883\tval-logloss:0.519598\n",
      "[209]\ttrain-logloss:0.492695\tval-logloss:0.519461\n",
      "[210]\ttrain-logloss:0.49249\tval-logloss:0.519548\n",
      "[211]\ttrain-logloss:0.492278\tval-logloss:0.519398\n",
      "[212]\ttrain-logloss:0.492097\tval-logloss:0.519263\n",
      "[213]\ttrain-logloss:0.49196\tval-logloss:0.519274\n",
      "[214]\ttrain-logloss:0.491884\tval-logloss:0.519264\n",
      "[215]\ttrain-logloss:0.491694\tval-logloss:0.519144\n",
      "[216]\ttrain-logloss:0.491627\tval-logloss:0.519116\n",
      "[217]\ttrain-logloss:0.491433\tval-logloss:0.519049\n",
      "[218]\ttrain-logloss:0.491252\tval-logloss:0.518995\n",
      "[219]\ttrain-logloss:0.491063\tval-logloss:0.518982\n",
      "[220]\ttrain-logloss:0.490891\tval-logloss:0.519014\n",
      "[221]\ttrain-logloss:0.490707\tval-logloss:0.519011\n",
      "[222]\ttrain-logloss:0.490505\tval-logloss:0.519102\n",
      "[223]\ttrain-logloss:0.490371\tval-logloss:0.519135\n",
      "[224]\ttrain-logloss:0.490186\tval-logloss:0.519124\n",
      "[225]\ttrain-logloss:0.489989\tval-logloss:0.519048\n",
      "[226]\ttrain-logloss:0.48981\tval-logloss:0.518931\n",
      "[227]\ttrain-logloss:0.489649\tval-logloss:0.518837\n",
      "[228]\ttrain-logloss:0.489503\tval-logloss:0.518888\n",
      "[229]\ttrain-logloss:0.489347\tval-logloss:0.518808\n",
      "[230]\ttrain-logloss:0.48925\tval-logloss:0.518723\n",
      "[231]\ttrain-logloss:0.489146\tval-logloss:0.518763\n",
      "[232]\ttrain-logloss:0.488966\tval-logloss:0.51875\n",
      "[233]\ttrain-logloss:0.488868\tval-logloss:0.518674\n",
      "[234]\ttrain-logloss:0.488703\tval-logloss:0.518661\n",
      "[235]\ttrain-logloss:0.488643\tval-logloss:0.518695\n",
      "[236]\ttrain-logloss:0.488404\tval-logloss:0.518567\n",
      "[237]\ttrain-logloss:0.488249\tval-logloss:0.518483\n",
      "[238]\ttrain-logloss:0.48812\tval-logloss:0.518394\n",
      "[239]\ttrain-logloss:0.487936\tval-logloss:0.518183\n",
      "[240]\ttrain-logloss:0.487761\tval-logloss:0.518065\n",
      "[241]\ttrain-logloss:0.487566\tval-logloss:0.518156\n",
      "[242]\ttrain-logloss:0.487368\tval-logloss:0.518093\n",
      "[243]\ttrain-logloss:0.48723\tval-logloss:0.51801\n",
      "[244]\ttrain-logloss:0.487191\tval-logloss:0.518025\n",
      "[245]\ttrain-logloss:0.487076\tval-logloss:0.517936\n",
      "[246]\ttrain-logloss:0.486875\tval-logloss:0.517801\n",
      "[247]\ttrain-logloss:0.486724\tval-logloss:0.517782\n",
      "[248]\ttrain-logloss:0.48656\tval-logloss:0.517579\n",
      "[249]\ttrain-logloss:0.486277\tval-logloss:0.517413\n",
      "[250]\ttrain-logloss:0.486105\tval-logloss:0.517441\n",
      "[251]\ttrain-logloss:0.485958\tval-logloss:0.517434\n",
      "[252]\ttrain-logloss:0.485748\tval-logloss:0.517371\n",
      "[253]\ttrain-logloss:0.485632\tval-logloss:0.517245\n",
      "[254]\ttrain-logloss:0.485558\tval-logloss:0.51722\n",
      "[255]\ttrain-logloss:0.485392\tval-logloss:0.517102\n",
      "[256]\ttrain-logloss:0.485254\tval-logloss:0.517031\n",
      "[257]\ttrain-logloss:0.485082\tval-logloss:0.516878\n",
      "[258]\ttrain-logloss:0.484938\tval-logloss:0.516827\n",
      "[259]\ttrain-logloss:0.484781\tval-logloss:0.516718\n",
      "[260]\ttrain-logloss:0.484603\tval-logloss:0.516796\n",
      "[261]\ttrain-logloss:0.484429\tval-logloss:0.516772\n",
      "[262]\ttrain-logloss:0.48428\tval-logloss:0.516731\n",
      "[263]\ttrain-logloss:0.484225\tval-logloss:0.516682\n",
      "[264]\ttrain-logloss:0.483986\tval-logloss:0.516322\n",
      "[265]\ttrain-logloss:0.483666\tval-logloss:0.516038\n",
      "[266]\ttrain-logloss:0.483508\tval-logloss:0.515957\n",
      "[267]\ttrain-logloss:0.483334\tval-logloss:0.515883\n",
      "[268]\ttrain-logloss:0.483164\tval-logloss:0.515844\n",
      "[269]\ttrain-logloss:0.483\tval-logloss:0.515813\n",
      "[270]\ttrain-logloss:0.482782\tval-logloss:0.515643\n",
      "[271]\ttrain-logloss:0.48261\tval-logloss:0.515651\n",
      "[272]\ttrain-logloss:0.482442\tval-logloss:0.515557\n",
      "[273]\ttrain-logloss:0.48227\tval-logloss:0.515542\n",
      "[274]\ttrain-logloss:0.482125\tval-logloss:0.515431\n",
      "[275]\ttrain-logloss:0.481949\tval-logloss:0.515435\n",
      "[276]\ttrain-logloss:0.481751\tval-logloss:0.515413\n",
      "[277]\ttrain-logloss:0.481619\tval-logloss:0.515353\n",
      "[278]\ttrain-logloss:0.481456\tval-logloss:0.51533\n",
      "[279]\ttrain-logloss:0.48129\tval-logloss:0.515167\n",
      "[280]\ttrain-logloss:0.481122\tval-logloss:0.515113\n",
      "[281]\ttrain-logloss:0.480971\tval-logloss:0.515082\n",
      "[282]\ttrain-logloss:0.480792\tval-logloss:0.514964\n",
      "[283]\ttrain-logloss:0.480665\tval-logloss:0.514873\n",
      "[284]\ttrain-logloss:0.480566\tval-logloss:0.514736\n",
      "[285]\ttrain-logloss:0.480407\tval-logloss:0.514668\n",
      "[286]\ttrain-logloss:0.480284\tval-logloss:0.514754\n",
      "[287]\ttrain-logloss:0.480184\tval-logloss:0.514685\n",
      "[288]\ttrain-logloss:0.480091\tval-logloss:0.514691\n",
      "[289]\ttrain-logloss:0.479921\tval-logloss:0.514576\n",
      "[290]\ttrain-logloss:0.479809\tval-logloss:0.514722\n",
      "[291]\ttrain-logloss:0.479672\tval-logloss:0.514733\n",
      "[292]\ttrain-logloss:0.479488\tval-logloss:0.514628\n",
      "[293]\ttrain-logloss:0.479317\tval-logloss:0.514643\n",
      "[294]\ttrain-logloss:0.47918\tval-logloss:0.514582\n",
      "[295]\ttrain-logloss:0.478865\tval-logloss:0.514336\n",
      "[296]\ttrain-logloss:0.4787\tval-logloss:0.514413\n",
      "[297]\ttrain-logloss:0.47854\tval-logloss:0.514365\n",
      "[298]\ttrain-logloss:0.478367\tval-logloss:0.514214\n",
      "[299]\ttrain-logloss:0.478221\tval-logloss:0.514188\n",
      "[300]\ttrain-logloss:0.478059\tval-logloss:0.514193\n",
      "[301]\ttrain-logloss:0.477968\tval-logloss:0.514163\n",
      "[302]\ttrain-logloss:0.477812\tval-logloss:0.514094\n",
      "[303]\ttrain-logloss:0.477638\tval-logloss:0.513977\n",
      "[304]\ttrain-logloss:0.477573\tval-logloss:0.51389\n",
      "[305]\ttrain-logloss:0.477486\tval-logloss:0.513847\n",
      "[306]\ttrain-logloss:0.477293\tval-logloss:0.513815\n",
      "[307]\ttrain-logloss:0.477029\tval-logloss:0.51356\n",
      "[308]\ttrain-logloss:0.476874\tval-logloss:0.513472\n",
      "[309]\ttrain-logloss:0.476691\tval-logloss:0.51348\n",
      "[310]\ttrain-logloss:0.476535\tval-logloss:0.513323\n",
      "[311]\ttrain-logloss:0.476416\tval-logloss:0.513323\n",
      "[312]\ttrain-logloss:0.476282\tval-logloss:0.513504\n",
      "[313]\ttrain-logloss:0.476208\tval-logloss:0.513478\n",
      "[314]\ttrain-logloss:0.476065\tval-logloss:0.513382\n",
      "[315]\ttrain-logloss:0.475927\tval-logloss:0.513305\n",
      "[316]\ttrain-logloss:0.475787\tval-logloss:0.51329\n",
      "[317]\ttrain-logloss:0.475639\tval-logloss:0.513211\n",
      "[318]\ttrain-logloss:0.475575\tval-logloss:0.513147\n",
      "[319]\ttrain-logloss:0.475474\tval-logloss:0.513212\n",
      "[320]\ttrain-logloss:0.475334\tval-logloss:0.513159\n",
      "[321]\ttrain-logloss:0.475147\tval-logloss:0.512979\n",
      "[322]\ttrain-logloss:0.474995\tval-logloss:0.512878\n",
      "[323]\ttrain-logloss:0.474864\tval-logloss:0.512902\n",
      "[324]\ttrain-logloss:0.474762\tval-logloss:0.512926\n",
      "[325]\ttrain-logloss:0.474599\tval-logloss:0.51302\n",
      "[326]\ttrain-logloss:0.474498\tval-logloss:0.512963\n",
      "[327]\ttrain-logloss:0.474356\tval-logloss:0.51288\n",
      "[328]\ttrain-logloss:0.474149\tval-logloss:0.512865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[329]\ttrain-logloss:0.473992\tval-logloss:0.512761\n",
      "[330]\ttrain-logloss:0.473801\tval-logloss:0.512782\n",
      "[331]\ttrain-logloss:0.473658\tval-logloss:0.51276\n",
      "[332]\ttrain-logloss:0.473529\tval-logloss:0.512732\n",
      "[333]\ttrain-logloss:0.473401\tval-logloss:0.512735\n",
      "[334]\ttrain-logloss:0.473251\tval-logloss:0.512724\n",
      "[335]\ttrain-logloss:0.4732\tval-logloss:0.512762\n",
      "[336]\ttrain-logloss:0.473129\tval-logloss:0.512743\n",
      "[337]\ttrain-logloss:0.473028\tval-logloss:0.512779\n",
      "[338]\ttrain-logloss:0.47288\tval-logloss:0.512637\n",
      "[339]\ttrain-logloss:0.472734\tval-logloss:0.512628\n",
      "[340]\ttrain-logloss:0.472579\tval-logloss:0.512675\n",
      "[341]\ttrain-logloss:0.472531\tval-logloss:0.512643\n",
      "[342]\ttrain-logloss:0.47244\tval-logloss:0.512663\n",
      "[343]\ttrain-logloss:0.472306\tval-logloss:0.512758\n",
      "[344]\ttrain-logloss:0.472213\tval-logloss:0.512821\n",
      "[345]\ttrain-logloss:0.472068\tval-logloss:0.512718\n",
      "[346]\ttrain-logloss:0.471981\tval-logloss:0.512778\n",
      "[347]\ttrain-logloss:0.471813\tval-logloss:0.512787\n",
      "[348]\ttrain-logloss:0.471665\tval-logloss:0.512778\n",
      "[349]\ttrain-logloss:0.471516\tval-logloss:0.512796\n",
      "Stopping. Best iteration:\n",
      "[339]\ttrain-logloss:0.472734\tval-logloss:0.512628\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#XGboost\n",
    "params = {\n",
    "    'eta': 0.4,\n",
    "    'max_depth': 3,\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss',\n",
    "    'seed': i,\n",
    "    'silent': False,\n",
    "    'nthreads':8\n",
    "}\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "watchlist = [(xgb.DMatrix(X_train, y_train), 'train'), (xgb.DMatrix(X_test, y_test), 'val')]\n",
    "model = xgb.train(params, dtrain, 1000, watchlist, verbose_eval=True, early_stopping_rounds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6219919\n",
      "[0.8838251  0.17228323 0.70242697 ... 0.82114273 0.9347503  0.52999765]\n"
     ]
    }
   ],
   "source": [
    "#Result\n",
    "testing_data = []\n",
    "with open('t3-test.txt') as f:\n",
    "    for line in f:\n",
    "        id1, id2 = map(int, line.split())\n",
    "        emb1 = embeddings[id1-1]\n",
    "        emb2 = embeddings[id2-1]\n",
    "        testing_data.append(np.hstack((emb1, emb2, alldate[id1-1], alldate[id2-1],\n",
    "                                      sims_title[id1-1][id2-1], sims_abs[id1-1][id2-1])))\n",
    "testing_data = np.array(testing_data)\n",
    "pred = model.predict(xgb.DMatrix(testing_data))\n",
    "mean = np.median(pred)\n",
    "print(mean)\n",
    "print(pred)\n",
    "pred[pred>=mean] = 1\n",
    "pred[pred<mean] = 0\n",
    "np.savetxt('pred.txt', pred.reshape(-1, 1), fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!python3 pred-txt-to-csv.py pred.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
